{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# HW3 Programming Assignment\n",
    "\n",
    "In this assignment, we will train LSTM POS-taggers, and evaluate their performance.\n",
    "\n",
    "We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "## I. Building a Basic POS Tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X367eCR3_x0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "### Preparing Data\n",
    "\n",
    "`train.txt`: The training data is present in this file. The file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "kFpH2P1A3_yG",
    "outputId": "1c889944-290e-4231-9b34-8c07f777aaf3"
   },
   "outputs": [],
   "source": [
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uojEDun83_yP",
    "outputId": "fb218599-7c4b-4b67-cf4d-929e2e8ce2d5"
   },
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "### Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 4\n",
    "HIDDEN_DIM = 8\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a hidden state. Then this hidden state is passed to a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCa30HQb3_ym"
   },
   "outputs": [],
   "source": [
    "class BasicPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding raw scores for tags (without softmax)\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training method\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate method\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuHjjH1rQeS"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 75% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP64WDReBuDr"
   },
   "source": [
    "Write a method to generate predictions for the validation set.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QgMHr7HCn1x"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions for val_data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions():\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    \n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors did the model make and why do you think it made them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Hyper-parameter Tuning\n",
    "\n",
    "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`. You will receive 50%/75%/100% credit for this section if your model, after being trained for 10 epochs, is able to achieve 80%/85%/90% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_EMBEDDING_DIM = None\n",
    "YOUR_HIDDEN_DIM = None\n",
    "YOUR_LEARNING_RATE = None\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## III. Character-level POS Tagger\n",
    "\n",
    "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word (treated as a tensor of characters, each mapped to character-index space) to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the BasicPOSTagger) to create a new word embedding that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n",
    "\n",
    "# New Hyperparameters\n",
    "EMBEDDING_DIM = 4\n",
    "HIDDEN_DIM = 8\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 10\n",
    "CHAR_EMBEDDING_DIM = 4\n",
    "CHAR_HIDDEN_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "class CharPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer for word: that maps words to their embedding space\n",
    "        # an embedding layer for character: that maps characters to their embedding space\n",
    "        # a character-level LSTM layer: that finds the character-level embedding for a word\n",
    "        # a word-level LSTM layer: that takes the concatenated embeddings as input and outputs hidden states\n",
    "        # a linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding raw scores for tags (without softmax)\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training method\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "\n",
    "def evaluate_char(model, loss_function):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate method\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-QttCw6Otf-"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train_char(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 85% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method to generate predictions for the validation set.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions for val_data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions():\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    \n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
